{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d95d02",
   "metadata": {},
   "source": [
    "## Predicting Fraudulent Transactions\n",
    "The aim of the project is to build a machine learning model to predict whether a transaction is fradulent based on transaction and identity features. \n",
    "\n",
    "\n",
    "## Why is this important?\n",
    "Predicting fraudulent transactions is important to protect customers by prevent such transactions from taking place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009e4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for machine learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "\n",
    "# for evaluation \n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d23413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_cols = ['TransactionID', 'C3', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V27', 'V28', 'V32', 'V98', 'V116', 'V117', 'V118', 'V119', 'V120', 'V153', 'V154', 'V157', 'V158', 'V235', 'V284', 'V286', 'V297', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V325', 'V327', 'V328', 'TransactionDT', 'TransactionAmt', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'V95', 'V96', 'V97', 'V99', 'V100', 'V101', 'V102', 'V126', 'V127', 'V145', 'V166', 'V279', 'V280', 'V285', 'V287', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V298', 'V299', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V326', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'R_emaildomain', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'ProductCD', 'isFraud']\n",
    "\n",
    "train_identity_cols = ['TransactionID', 'id_08', 'id_13', 'id_17', 'id_19', 'id_20', 'id_21', 'id_26', 'id_16', 'id_27', 'DeviceInfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca6a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_transaction = pd.read_csv(\"../data/train_transaction.csv\", usecols=train_transaction_cols)\n",
    "train_identity = pd.read_csv(\"../data/train_identity.csv\", usecols=train_identity_cols)\n",
    "\n",
    "# merge the dataframe\n",
    "dataframe = pd.merge(train_transaction,\n",
    "                     train_identity,\n",
    "                     how=\"left\",\n",
    "                     on=\"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe49d0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>...</th>\n",
       "      <th>id_08</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_16</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_21</th>\n",
       "      <th>id_26</th>\n",
       "      <th>id_27</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>166.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3  card5  addr1  ...  id_08 id_13     id_16  id_17  id_19  \\\n",
       "0    NaN  150.0  142.0  315.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "1  404.0  150.0  102.0  325.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "2  490.0  150.0  166.0  330.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "3  567.0  150.0  117.0  476.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "4  514.0  150.0  102.0  420.0  ...    NaN   NaN  NotFound  166.0  542.0   \n",
       "\n",
       "   id_20  id_21  id_26  id_27                     DeviceInfo  \n",
       "0    NaN    NaN    NaN    NaN                            NaN  \n",
       "1    NaN    NaN    NaN    NaN                            NaN  \n",
       "2    NaN    NaN    NaN    NaN                            NaN  \n",
       "3    NaN    NaN    NaN    NaN                            NaN  \n",
       "4  144.0    NaN    NaN    NaN  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be355716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numeric features to object\n",
    "# to follow the documentation \n",
    "numeric_cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\", \"id_13\",\n",
    "                    \"id_17\", \"id_19\", \"id_20\", \"id_21\", \"id_26\"]\n",
    "\n",
    "cat_cols = list(dataframe.select_dtypes(include=['object']).columns)\n",
    "cat_cols += numeric_cat_cols \n",
    "\n",
    "dataframe[cat_cols] = dataframe[cat_cols].astype(\"O\")\n",
    "\n",
    "# collecting all of the numerical features \n",
    "num_cols = [x for x in dataframe.select_dtypes(include=['number']).columns\n",
    "            if x not in cat_cols\n",
    "            if x not in \"isFraud\"]\n",
    "\n",
    "# collecting all of the features with missing values\n",
    "features_with_na = [x for x in dataframe.columns if dataframe[x].isnull().sum() > 0]\n",
    "\n",
    "# determine percentage of missing values (expressed as decimals)\n",
    "# and display the result ordered by % of missin data\n",
    "missing_vals_df = pd.DataFrame(\n",
    "    dataframe[features_with_na].isnull().mean().sort_values(ascending=False),\n",
    "    columns=[\"percentage\"]\n",
    ")\n",
    "\n",
    "# seperating features with missing\n",
    "# values to categorical and numeric\n",
    "cat_feat_with_na = [x for x in cat_cols if x in features_with_na]\n",
    "num_feat_with_na = [x for x in num_cols if x in features_with_na]\n",
    "\n",
    "# features to remove\n",
    "drop_features = [col for col in \n",
    "                 list(missing_vals_df.iloc[np.where(missing_vals_df.loc[cat_feat_with_na, 'percentage'] > 0.5)].index)]\n",
    "\n",
    "drop_features += [col for col in\n",
    "                  list(missing_vals_df.iloc[np.where(missing_vals_df.loc[num_feat_with_na, 'percentage'] > 0.5)].index)]\n",
    "\n",
    "# defining the different features \n",
    "discrete_features = [x for x in num_cols \n",
    "                     if len(dataframe[x].unique()) < 20\n",
    "                     and x not in drop_features]\n",
    "\n",
    "continuous_features = [x for x in num_cols\n",
    "                       if x not in discrete_features + drop_features + [\"TransactionID\"]]\n",
    "\n",
    "high_cardinality_cats = [\"R_emaildomain\", \"card1\", \"card2\", \"card3\",\n",
    "                         \"card5\", \"addr1\", \"addr2\", \"id_13\", \"id_17\", \"id_19\", \"id_20\",\n",
    "                         \"id_21\", \"id_26\"]\n",
    "\n",
    "categorical_features = [x for x in cat_cols if x not in drop_features + high_cardinality_cats]\n",
    "\n",
    "all_features = discrete_features + continuous_features + high_cardinality_cats + categorical_features\n",
    "\n",
    "impute_freq = high_cardinality_cats + categorical_features + discrete_features\n",
    "\n",
    "cat_codes_cols = high_cardinality_cats + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21a27c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class AggregateCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Reduces the cardinality of categorical features\n",
    "    Credit: Raj Sangani- https://bit.ly/3BSxdTX\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features: str or list\n",
    "        The feature(s) with high cardinality that we want to\n",
    "        aggregate\n",
    "\n",
    "    threshold: float\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    fit:\n",
    "        The transformer will not learn from any parameter\n",
    "\n",
    "    transform:\n",
    "        Drops the explicitly selected features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features: List[Union[str, int]], threshold: float = 0.75):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "        self.features = features\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        # find the threshold value using the percentage and number of instances\n",
    "        threshold_value = int(self.threshold * len(X))\n",
    "        df = X.copy()\n",
    "\n",
    "        for col in self.features:\n",
    "            counts = Counter(df[col])\n",
    "            s = 0\n",
    "            category_list = []\n",
    "            # loop through the category name and its corresponding frequency\n",
    "            for i, j in counts.most_common():\n",
    "                # add the frequency to the sum\n",
    "                s += counts[i]\n",
    "                category_list.append(i)\n",
    "\n",
    "                if s >= threshold_value:\n",
    "                    break\n",
    "            # replace all instances not in our new categories by other\n",
    "            category_list.append(\"other\")\n",
    "            df[col] = df[col].apply(lambda x: x if x in category_list else \"other\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A wrapper around `SimpleImputer` to return data frames with columns.\n",
    "    Credit: https://bit.ly/3r2N40k\n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "            \n",
    "        self.features = features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer_dict_ = {}\n",
    "        for feature in self.features:\n",
    "            self.imputer_dict_[feature] = X[feature].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.features:\n",
    "            X[feature] = X[feature].fillna(self.imputer_dict_[feature])\n",
    "        return X\n",
    "    \n",
    "class MeanImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Numerical missing value imputer.\n",
    "    Credit: https://bit.ly/3r2N40k\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "        \n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # persist mode in a dictionary\n",
    "        self.imputer_dict_ = {}\n",
    "        for feature in self.features:\n",
    "            self.imputer_dict_[feature] = X[feature].mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.features:\n",
    "            X[feature].fillna(self.imputer_dict_[feature], inplace=True)\n",
    "        return X\n",
    "    \n",
    "class CategoryConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "            \n",
    "        self.features = features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.codes_ = {}\n",
    "        for feature in self.features:\n",
    "            X[feature] = X[feature].astype(\"category\")\n",
    "            self.codes_[feature] = dict(zip(X[feature].values, X[feature].cat.codes))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.features: \n",
    "            X[feature] = X[feature].map(self.codes_[feature])\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f70c330",
   "metadata": {},
   "source": [
    "### Building a classifier without an ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8efc4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ROC_AUC:1.0\n",
      "test ROC_AUC: 0.912609226302252\n"
     ]
    }
   ],
   "source": [
    "df1_ = dataframe.copy()\n",
    "\n",
    "# impute mode to high cardinality, categorical features,\n",
    "# and discrete features\n",
    "imp_most_frequent = MostFrequentImputer(features=impute_freq)\n",
    "df1_ = imp_most_frequent.fit_transform(df1_)\n",
    "\n",
    "# aggregate the high cardinality categoricals so they have less dimensions\n",
    "aggregate_categoricals = AggregateCategorical(high_cardinality_cats)\n",
    "df1_ = aggregate_categoricals.fit_transform(df1_)\n",
    "\n",
    "# convert all categorical features to type category and use codes\n",
    "transform_dtype = CategoryConverter(features=cat_codes_cols)\n",
    "df1_ = transform_dtype.fit_transform(df1_)\n",
    "\n",
    "# impute the mean to the continuous features\n",
    "imp_mean = MeanImputer(features=continuous_features)\n",
    "df1_ = imp_mean.fit_transform(df1_)\n",
    "\n",
    "X = df1_[all_features]\n",
    "y = df1_[\"isFraud\"]\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    stratify=y,\n",
    "    random_state=25\n",
    ")\n",
    "\n",
    "# modelling\n",
    "random_forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=25)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "train_preds = random_forest.predict_proba(X_train)\n",
    "y_preds = random_forest.predict_proba(X_test)\n",
    "\n",
    "preds_roc_auc_ = roc_auc_score(y_test, y_preds[:, 1])\n",
    "train_roc_auc_ = roc_auc_score(y_train, train_preds[:, 1])\n",
    "\n",
    "print(f\"train ROC_AUC:{train_roc_auc_}\\n\\\n",
    "test ROC_AUC: {preds_roc_auc_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca62979",
   "metadata": {},
   "source": [
    "### Building a classifier with ML Pipeline\n",
    "Using `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec49e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline transformations\n",
    "process_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"most_frequent_imputer\", \n",
    "            MostFrequentImputer(features=impute_freq)\n",
    "        ), \n",
    "        (\n",
    "            \"aggregate_high_cardinality_features\",\n",
    "            AggregateCategorical(features=high_cardinality_cats),\n",
    "        ), \n",
    "        (\n",
    "            \"get_categorical_codes\",\n",
    "            CategoryConverter(features=cat_codes_cols),\n",
    "        ),\n",
    "        (\n",
    "            \"mean_imputer\",\n",
    "            MeanImputer(features=continuous_features),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d89e7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ROC_AUC:1.0\n",
      "test ROC_AUC: 0.912609226302252\n"
     ]
    }
   ],
   "source": [
    "df2_ = dataframe.copy()\n",
    "\n",
    "# seperate training features and target features\n",
    "# perform all processing to training features\n",
    "X = process_pipe.fit_transform(df2_[all_features])\n",
    "y = df2_[\"isFraud\"]\n",
    "\n",
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    stratify=y,\n",
    "    random_state=25\n",
    ")\n",
    "\n",
    "# modelling\n",
    "random_forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=25)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "train_preds = random_forest.predict_proba(X_train)\n",
    "y_preds = random_forest.predict_proba(X_test)\n",
    "\n",
    "preds_roc_auc_ = roc_auc_score(y_test, y_preds[:, 1])\n",
    "train_roc_auc_ = roc_auc_score(y_train, train_preds[:, 1])\n",
    "\n",
    "print(f\"train ROC_AUC:{train_roc_auc_}\\n\\\n",
    "test ROC_AUC: {preds_roc_auc_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
